(window.webpackJsonp=window.webpackJsonp||[]).push([[80],{1045:function(a,t,s){a.exports=s.p+"assets/img/image-20230927233329228.73cc2d55.png"},1513:function(a,t,s){"use strict";s.r(t);var e=s(4),r=Object(e.a)({},(function(){var a=this,t=a._self._c;return t("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[t("h1",{attrs:{id:"第-1-章-kafka-硬件配置选择"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#第-1-章-kafka-硬件配置选择"}},[a._v("#")]),a._v(" 第 1 章 Kafka 硬件配置选择")]),a._v(" "),t("h2",{attrs:{id:"_1-1-场景说明"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-1-场景说明"}},[a._v("#")]),a._v(" 1.1 场景说明")]),a._v(" "),t("p",[a._v("100 万日活，每人每天 100 条日志，每天总共的日志条数是 100 万 * 100 条 = 1 亿条。")]),a._v(" "),t("p",[a._v("1 亿/24 小时/60 分/60 秒 = 1150 条/每秒钟。")]),a._v(" "),t("p",[a._v("每条日志大小：0.5k - 2k（取 1k）。")]),a._v(" "),t("p",[a._v("1150 条/每秒钟 * 1k ≈ 1m/s 。")]),a._v(" "),t("p",[a._v("高峰期每秒钟：1150 条 * 20 倍 = 23000 条。")]),a._v(" "),t("p",[a._v("每秒多少数据量：20MB/s。")]),a._v(" "),t("h2",{attrs:{id:"_1-2-服务器台数选择"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-2-服务器台数选择"}},[a._v("#")]),a._v(" 1.2 服务器台数选择")]),a._v(" "),t("p",[a._v("服务器台数 = 2 _ （生产者峰值生产速率 _ 副本 / 100） + 1")]),a._v(" "),t("p",[a._v("​ = 2 _ （20m/s _ 2 / 100） + 1")]),a._v(" "),t("p",[a._v("​ = 3 台")]),a._v(" "),t("p",[a._v("建议 3 台服务器。")]),a._v(" "),t("h2",{attrs:{id:"_1-3-磁盘选择"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-3-磁盘选择"}},[a._v("#")]),a._v(" 1.3 磁盘选择")]),a._v(" "),t("p",[a._v("kafka 底层主要是"),t("strong",[a._v("顺序写")]),a._v("，固态硬盘和机械硬盘的顺序写速度差不多。")]),a._v(" "),t("p",[a._v("建议选择普通的机械硬盘。")]),a._v(" "),t("p",[a._v("每天总数据量：1 亿条 * 1k ≈ 100g")]),a._v(" "),t("p",[a._v("100g _ 副本 2 _ 保存时间 3 天 / 0.7 ≈ 1T")]),a._v(" "),t("p",[a._v("建议三台服务器硬盘总大小，大于等于 1T。")]),a._v(" "),t("h2",{attrs:{id:"_1-4-内存选择"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-4-内存选择"}},[a._v("#")]),a._v(" 1.4 内存选择")]),a._v(" "),t("p",[a._v("Kafka 内存组成：堆内存 + 页缓存")]),a._v(" "),t("p",[a._v("1）Kafka 堆内存建议每个节点：10g ~ 15")]),a._v(" "),t("p",[a._v("在 kafka-server-start.sh 中修改")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v('if [ "x$KAFKA_HEAP_OPTS" = "x" ]; then\n export KAFKA_HEAP_OPTS="-Xmx10G -Xms10G"\nfi\n')])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br")])]),t("p",[a._v("（1）查看 Kafka 进程号")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop102 kafka]$ jps\n2321 Kafka\n5255 Jps\n1931 QuorumPeerMain\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br")])]),t("p",[a._v("（2）根据 Kafka 进程号，查看 Kafka 的 GC 情")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop102 kafka]$ jstat -gc 2321 1s 10\nS0C S1C S0U S1U EC EU OC OU MC MU CCSC CCSU YGC YGCT FGC FGCT GCT\n0.0 7168.0 0.0 7168.0 103424.0 60416.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 60416.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 60416.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 60416.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 60416.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 61440.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 61440.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 61440.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 61440.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n0.0 7168.0 0.0 7168.0 103424.0 61440.0 1986560.0 148433.5 52092.0 46656.1 6780.0 6202.2 13 0.531 0 0.000 0.531\n\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br"),t("span",{staticClass:"line-number"},[a._v("12")]),t("br"),t("span",{staticClass:"line-number"},[a._v("13")]),t("br")])]),t("p",[a._v("参数说明：")]),a._v(" "),t("p",[a._v("S0C：第一个幸存区的大小；")]),a._v(" "),t("p",[a._v("S1C：第二个幸存区的大小")]),a._v(" "),t("p",[a._v("S0U：第一个幸存区的使用大小；")]),a._v(" "),t("p",[a._v("S1U：第二个幸存区的使用大小")]),a._v(" "),t("p",[a._v("EC：伊甸园区的大小；")]),a._v(" "),t("p",[a._v("EU：伊甸园区的使用大小")]),a._v(" "),t("p",[a._v("OC：老年代大小；")]),a._v(" "),t("p",[a._v("OU：老年代使用大小")]),a._v(" "),t("p",[a._v("MC：方法区大小；")]),a._v(" "),t("p",[a._v("MU：方法区使用大小")]),a._v(" "),t("p",[a._v("CCSC:压缩类空间大小；")]),a._v(" "),t("p",[a._v("CCSU:压缩类空间使用大小")]),a._v(" "),t("p",[a._v("YGC：年轻代垃圾回收次数；")]),a._v(" "),t("p",[a._v("YGCT：年轻代垃圾回收消耗时间")]),a._v(" "),t("p",[a._v("FGC：老年代垃圾回收次数；")]),a._v(" "),t("p",[a._v("FGCT：老年代垃圾回收消耗时间")]),a._v(" "),t("p",[a._v("GCT：垃圾回收消耗总时间；")]),a._v(" "),t("p",[a._v("（3）根据 Kafka 进程号，查看 Kafka 的堆内存")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop102 kafka]$ jmap -heap 2321\nAttaching to process ID 2321, please wait...\nDebugger attached successfully.\nServer compiler detected.\nJVM version is 25.212-b10\n\nusing thread-local object allocation.\nGarbage-First (G1) GC with 8 thread(s)\n\nHeap Configuration:\n MinHeapFreeRatio = 40\n MaxHeapFreeRatio = 70\n MaxHeapSize = 2147483648 (2048.0MB)\n NewSize = 1363144 (1.2999954223632812MB)\n MaxNewSize = 1287651328 (1228.0MB)\n OldSize = 5452592 (5.1999969482421875MB)\n NewRatio = 2\n SurvivorRatio = 8\n MetaspaceSize = 21807104 (20.796875MB)\n CompressedClassSpaceSize = 1073741824 (1024.0MB)\n MaxMetaspaceSize = 17592186044415 MB\n G1HeapRegionSize = 1048576 (1.0MB)\n\nHeap Usage:\nG1 Heap:\n regions = 2048\n capacity = 2147483648 (2048.0MB)\n used = 246367744 (234.95458984375MB)\n free = 1901115904 (1813.04541015625MB)\n 11.472392082214355% used\nG1 Young Generation:\nEden Space:\n regions = 83\n capacity = 105906176 (101.0MB)\n used = 87031808 (83.0MB)\n free = 18874368 (18.0MB)\n 82.17821782178218% used\nSurvivor Space:\n regions = 7\n capacity = 7340032 (7.0MB)\n used = 7340032 (7.0MB)\n free = 0 (0.0MB)\n 100.0% used\nG1 Old Generation:\n regions = 147\n capacity = 2034237440 (1940.0MB)\n used = 151995904 (144.95458984375MB)\n free = 1882241536 (1795.04541015625MB)\n 7.471886074420103% used\n\n13364 interned Strings occupying 1449608 bytes.\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br"),t("span",{staticClass:"line-number"},[a._v("12")]),t("br"),t("span",{staticClass:"line-number"},[a._v("13")]),t("br"),t("span",{staticClass:"line-number"},[a._v("14")]),t("br"),t("span",{staticClass:"line-number"},[a._v("15")]),t("br"),t("span",{staticClass:"line-number"},[a._v("16")]),t("br"),t("span",{staticClass:"line-number"},[a._v("17")]),t("br"),t("span",{staticClass:"line-number"},[a._v("18")]),t("br"),t("span",{staticClass:"line-number"},[a._v("19")]),t("br"),t("span",{staticClass:"line-number"},[a._v("20")]),t("br"),t("span",{staticClass:"line-number"},[a._v("21")]),t("br"),t("span",{staticClass:"line-number"},[a._v("22")]),t("br"),t("span",{staticClass:"line-number"},[a._v("23")]),t("br"),t("span",{staticClass:"line-number"},[a._v("24")]),t("br"),t("span",{staticClass:"line-number"},[a._v("25")]),t("br"),t("span",{staticClass:"line-number"},[a._v("26")]),t("br"),t("span",{staticClass:"line-number"},[a._v("27")]),t("br"),t("span",{staticClass:"line-number"},[a._v("28")]),t("br"),t("span",{staticClass:"line-number"},[a._v("29")]),t("br"),t("span",{staticClass:"line-number"},[a._v("30")]),t("br"),t("span",{staticClass:"line-number"},[a._v("31")]),t("br"),t("span",{staticClass:"line-number"},[a._v("32")]),t("br"),t("span",{staticClass:"line-number"},[a._v("33")]),t("br"),t("span",{staticClass:"line-number"},[a._v("34")]),t("br"),t("span",{staticClass:"line-number"},[a._v("35")]),t("br"),t("span",{staticClass:"line-number"},[a._v("36")]),t("br"),t("span",{staticClass:"line-number"},[a._v("37")]),t("br"),t("span",{staticClass:"line-number"},[a._v("38")]),t("br"),t("span",{staticClass:"line-number"},[a._v("39")]),t("br"),t("span",{staticClass:"line-number"},[a._v("40")]),t("br"),t("span",{staticClass:"line-number"},[a._v("41")]),t("br"),t("span",{staticClass:"line-number"},[a._v("42")]),t("br"),t("span",{staticClass:"line-number"},[a._v("43")]),t("br"),t("span",{staticClass:"line-number"},[a._v("44")]),t("br"),t("span",{staticClass:"line-number"},[a._v("45")]),t("br"),t("span",{staticClass:"line-number"},[a._v("46")]),t("br"),t("span",{staticClass:"line-number"},[a._v("47")]),t("br"),t("span",{staticClass:"line-number"},[a._v("48")]),t("br"),t("span",{staticClass:"line-number"},[a._v("49")]),t("br"),t("span",{staticClass:"line-number"},[a._v("50")]),t("br"),t("span",{staticClass:"line-number"},[a._v("51")]),t("br")])]),t("p",[a._v("2）页缓存：页缓存是 Linux 系统服务器的内存**。我们只需要保证 1 个 segment（1g）中 25%**的数据在内存中就好。")]),a._v(" "),t("p",[a._v("每个节点页缓存大小 =（分区数 _ 1g _ 25%）/ 节点数。例如 10 个分区，页缓存大小 =（10 _ 1g _ 25%）/ 3 ≈ 1g")]),a._v(" "),t("p",[a._v("建议服务器内存大于等于 11G。")]),a._v(" "),t("h2",{attrs:{id:"_1-5-cpu-选择"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-5-cpu-选择"}},[a._v("#")]),a._v(" 1.5 CPU 选择")]),a._v(" "),t("ul",[t("li",[t("p",[t("code",[a._v("num.io.threads = 8")]),a._v(" 负责写磁盘的线程数，整个参数值要占总核数的 50%。")])]),a._v(" "),t("li",[t("p",[t("code",[a._v("num.replica.fetchers = 1")]),a._v(" 副本拉取线程数，这个参数占总核数的 50%的 1/3。")])]),a._v(" "),t("li",[t("p",[t("code",[a._v("num.network.threads = 3")]),a._v(" 数据传输线程数，这个参数占总核数的 50%的 2/3。")])])]),a._v(" "),t("p",[a._v("建议 32 个 cpu core。")]),a._v(" "),t("h2",{attrs:{id:"_1-6-网络选择"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-6-网络选择"}},[a._v("#")]),a._v(" 1.6 网络选择")]),a._v(" "),t("p",[a._v("网络带宽 = 峰值吞吐量 ≈ 20MB/s 选择千兆网卡即可。")]),a._v(" "),t("p",[a._v("100Mbps 单位是 bit；10M/s 单位是 byte ; 1byte = 8bit，100Mbps/8 = 12.5M/s。")]),a._v(" "),t("p",[a._v("一般百兆的网卡（100Mbps ）、千兆的网卡（1000Mbps）、万兆的网卡（10000Mbps）。")]),a._v(" "),t("h1",{attrs:{id:"第-2-章-kafka-生产者"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#第-2-章-kafka-生产者"}},[a._v("#")]),a._v(" 第 2 章 Kafka 生产者")]),a._v(" "),t("blockquote",[t("p",[a._v("3.1.1 Updating Broker Configs")]),a._v(" "),t("p",[a._v("From Kafka version 1.1 onwards, some of the broker configs can be updated without restarting the broker. See the Dynamic Update Mode column in Broker Configs for the update mode of each broker config.")]),a._v(" "),t("p",[t("strong",[a._v("read-only")]),a._v(": Requires a broker restart for update")]),a._v(" "),t("p",[t("strong",[a._v("per-broker")]),a._v(": May be updated dynamically for each broker")]),a._v(" "),t("p",[t("strong",[a._v("cluster-wide")]),a._v(": May be updated dynamically as a cluster-wide default. May also be updated as a per-broker value for testing.")]),a._v(" "),t("p",[a._v("May also be updated as a per-broker value for testing.")])]),a._v(" "),t("h2",{attrs:{id:"_2-1-kafka-生产者核心参数配置"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-1-kafka-生产者核心参数配置"}},[a._v("#")]),a._v(" 2.1 Kafka 生产者核心参数配置")]),a._v(" "),t("p",[t("img",{attrs:{src:s(257),alt:"发送流程"}})]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("参数名称")]),a._v(" "),t("th",[a._v("描述")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("bootstrap.servers")]),a._v(" "),t("td",[a._v("生产者连接集群所需的 broker 地 址清单 。 例 如 hadoop102:9092,hadoop103:9092,hadoop104:9092，可以设置 1 个或者多个，中间用逗号隔开。注意这里并"),t("strong",[a._v("非需要所有的 broker 地址")]),a._v("，因为生产者从给定的 broker 里查找到其他 broker 信息。")])]),a._v(" "),t("tr",[t("td",[a._v("key.serializer 和 value.serializer")]),a._v(" "),t("td",[a._v("指定发送消息的 key 和 value 的序列化类型。一定要写 全类名。")])]),a._v(" "),t("tr",[t("td",[a._v("buffer.memory")]),a._v(" "),t("td",[a._v("RecordAccumulator 缓冲区总大小，"),t("strong",[a._v("默认 32m。")])])]),a._v(" "),t("tr",[t("td",[a._v("batch.size")]),a._v(" "),t("td",[a._v("缓冲区一批数据最大值，默认 16k。适当增加该值，可 以提高吞吐量，但是如果该值设置太大，会导致数据 传输延迟增加。")])]),a._v(" "),t("tr",[t("td",[a._v("linger.ms")]),a._v(" "),t("td",[a._v("如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，"),t("strong",[a._v("默认值是 0ms")]),a._v("，表示没有延迟。生产环境建议该值大小为 5-100ms 之间。")])]),a._v(" "),t("tr",[t("td",[a._v("acks")]),a._v(" "),t("td",[a._v("0：生产者发送过来的数据，不需要等数据落盘应答。 1：生产者发送过来的数据，Leader 收到数据后应答。 -1（all）：生产者发送过来的数据，Leader+和 isr 队列里面的所有节点收齐数据后应答。"),t("strong",[a._v("默认值是-1，-1 和 all 是等价的。")])])]),a._v(" "),t("tr",[t("td",[a._v("max.in.flight.requests.per.connection")]),a._v(" "),t("td",[a._v("允许最多没有返回 ack 的次数，"),t("strong",[a._v("默认为 5")]),a._v("，开启幂等性要保证该值是 1-5 的数字。")])]),a._v(" "),t("tr",[t("td",[a._v("retries")]),a._v(" "),t("td",[a._v("当消息发送出现错误的时候，系统会重发消息。retries 表示重试次数。"),t("strong",[a._v("默认是 int 最大值，2147483647")]),a._v("。 如果设置了重试，还想保证消息的有序性，需要设置 MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION=1 否则在重试此失败消息的时候，其他的消息可能发送成功了。")])]),a._v(" "),t("tr",[t("td",[a._v("retry.backoff.ms")]),a._v(" "),t("td",[a._v("两次重试之间的时间间隔，默认是 100ms。")])]),a._v(" "),t("tr",[t("td",[a._v("enable.idempotence")]),a._v(" "),t("td",[a._v("是否开启幂等性，"),t("strong",[a._v("默认 true")]),a._v("，开启幂等性。")])]),a._v(" "),t("tr",[t("td",[a._v("compression.type")]),a._v(" "),t("td",[a._v("生产者发送的所有数据的压缩方式。"),t("strong",[a._v("默认是 none")]),a._v("，也就是不压缩。 支持压缩类型："),t("strong",[a._v("none、gzip、snappy、lz4 和 zstd。")])])])])]),a._v(" "),t("h2",{attrs:{id:"_2-2-生产者如何提高吞吐量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-2-生产者如何提高吞吐量"}},[a._v("#")]),a._v(" 2.2 生产者如何提高吞吐量")]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("参数名称")]),a._v(" "),t("th",[a._v("描述")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("buffer.memory")]),a._v(" "),t("td",[a._v("RecordAccumulator 缓冲区总大小，默认 "),t("strong",[a._v("32m。")])])]),a._v(" "),t("tr",[t("td",[a._v("batch.size")]),a._v(" "),t("td",[a._v("缓冲区一批数据最大值，默认 16k。适当增加该值，可 以提高吞吐量，但是如果该值设置太大，会导致数据传 输延迟增加。")])]),a._v(" "),t("tr",[t("td",[a._v("linger.ms")]),a._v(" "),t("td",[a._v("如果数据迟迟未达到 batch.size，sender 等待 linger.time 之后就会发送数据。单位 ms，"),t("strong",[a._v("默认值是 0ms")]),a._v("，表示没有 延迟。生产环境建议该值大小为 5-100ms 之间。")])]),a._v(" "),t("tr",[t("td",[a._v("compression.type")]),a._v(" "),t("td",[a._v("生产者发送的所有数据的压缩方式。"),t("strong",[a._v("默认是 none")]),a._v("，也就 是不压缩。 支持压缩类型："),t("strong",[a._v("none、gzip、snappy、lz4 和 zstd。")])])])])]),a._v(" "),t("h2",{attrs:{id:"_2-3-数据可靠性"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-3-数据可靠性"}},[a._v("#")]),a._v(" 2.3 数据可靠性")]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("参数名称")]),a._v(" "),t("th",[a._v("描述")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("acks")]),a._v(" "),t("td",[a._v("0：生产者发送过来的数据，不需要等数据落盘应答。"),t("br"),a._v("1：生产者发送过来的数据，Leader 收到数据后应答。"),t("br"),a._v("-1（all）：生产者发送过来的数据，Leader+和 isr 队列 里面的所有节点收齐数据后应答。"),t("strong",[a._v("默认值是-1，-1 和 all 是等价的。")])])])])]),a._v(" "),t("ul",[t("li",[a._v("至少一次（At Least Once）= ACK 级别设置为-1 + 分区副本大于等于 2 + ISR 里应答的最小副本数量大于等于 2")])]),a._v(" "),t("h2",{attrs:{id:"_2-4-数据去重"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-4-数据去重"}},[a._v("#")]),a._v(" 2.4 数据去重")]),a._v(" "),t("p",[a._v("1）配置参数")]),a._v(" "),t("p",[t("code",[a._v("enable.idempotence")]),a._v(" : 是否开启幂等性，"),t("strong",[a._v("默认 true")]),a._v("，表示开启幂等性。")]),a._v(" "),t("p",[a._v("2）Kafka 的事务一共有如下 5 个 API")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("// 1 初始化事务\nvoid initTransactions();\n// 2 开启事务\nvoid beginTransaction() throws ProducerFencedException;\n// 3 在事务内提交已经消费的偏移量（主要用于消费者）\nvoid sendOffsetsToTransaction(Map<TopicPartition, OffsetAndMetadata> offsets,\n String consumerGroupId) throws\nProducerFencedException;\n// 4 提交事务\nvoid commitTransaction() throws ProducerFencedException;\n// 5 放弃事务（类似于回滚事务的操作）\nvoid abortTransaction() throws ProducerFencedException;\n\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br"),t("span",{staticClass:"line-number"},[a._v("3")]),t("br"),t("span",{staticClass:"line-number"},[a._v("4")]),t("br"),t("span",{staticClass:"line-number"},[a._v("5")]),t("br"),t("span",{staticClass:"line-number"},[a._v("6")]),t("br"),t("span",{staticClass:"line-number"},[a._v("7")]),t("br"),t("span",{staticClass:"line-number"},[a._v("8")]),t("br"),t("span",{staticClass:"line-number"},[a._v("9")]),t("br"),t("span",{staticClass:"line-number"},[a._v("10")]),t("br"),t("span",{staticClass:"line-number"},[a._v("11")]),t("br"),t("span",{staticClass:"line-number"},[a._v("12")]),t("br"),t("span",{staticClass:"line-number"},[a._v("13")]),t("br")])]),t("h2",{attrs:{id:"_2-5-数据有序"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-5-数据有序"}},[a._v("#")]),a._v(" 2.5 数据有序")]),a._v(" "),t("p",[a._v("单分区内，有序（有条件的，不能乱序）；多分区，分区与分区间无序；")]),a._v(" "),t("h2",{attrs:{id:"_2-6-数据乱序"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-6-数据乱序"}},[a._v("#")]),a._v(" 2.6 数据乱序")]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("参数名称")]),a._v(" "),t("th",[a._v("描述")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("enable.idempotence")]),a._v(" "),t("td",[a._v("是否开启幂等性，"),t("strong",[a._v("默认 true")]),a._v("，表示开启幂等性。")])]),a._v(" "),t("tr",[t("td",[a._v("max.in.flight.requests.per.connection")]),a._v(" "),t("td",[a._v("允许最多没有返回 ack 的次数，"),t("strong",[a._v("默认为 5")]),a._v("，开启幂等性要保证该值是 1-5 的数字。")])])])]),a._v(" "),t("h1",{attrs:{id:"第-3-章-kafka-broker"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#第-3-章-kafka-broker"}},[a._v("#")]),a._v(" 第 3 章 Kafka Broker")]),a._v(" "),t("h2",{attrs:{id:"_3-1-broker-核心参数配置"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-broker-核心参数配置"}},[a._v("#")]),a._v(" 3.1 Broker 核心参数配置")]),a._v(" "),t("p",[t("img",{attrs:{src:s(258),alt:"Kafka Broker总体工作流程"}})]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("参数名称")]),a._v(" "),t("th",[a._v("描述")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("replica.lag.time.max.ms")]),a._v(" "),t("td",[a._v("ISR 中，如果 Follower 长时间未向 Leader 发送通信请求或同步数据，则该 Follower 将被踢出 ISR。 该时间阈值，"),t("strong",[a._v("默认 30s。")])])]),a._v(" "),t("tr",[t("td",[a._v("auto.leader.rebalance.enable")]),a._v(" "),t("td",[t("strong",[a._v("默认是 true")]),a._v("。 自动 Leader Partition 平衡。")])]),a._v(" "),t("tr",[t("td",[a._v("leader.imbalance.per.broker.percentage")]),a._v(" "),t("td",[t("strong",[a._v("默认是 10%。")]),a._v(" 每个 broker 允许的不平衡的 leader 的比率。如果每个 broker 超过了这个值，控制器 会触发 leader 的平衡。")])]),a._v(" "),t("tr",[t("td",[a._v("leader.imbalance.check.interval.seconds")]),a._v(" "),t("td",[t("strong",[a._v("默认值 300 秒")]),a._v("。检查 leader 负载是否平衡的间隔时间。")])]),a._v(" "),t("tr",[t("td",[a._v("log.segment.bytes")]),a._v(" "),t("td",[a._v("Kafka 中 log 日志是分成一块块存储的，此配置是指 log 日志划分成块的大小，"),t("strong",[a._v("默认值 1G。")])])]),a._v(" "),t("tr",[t("td",[a._v("log.index.interval.bytes")]),a._v(" "),t("td",[t("strong",[a._v("默认 4kb")]),a._v("，kafka 里面每当写入了 4kb 大小的日志 （.log），然后就往 index 文件里面记录一个索引。")])]),a._v(" "),t("tr",[t("td",[a._v("log.retention.hours")]),a._v(" "),t("td",[a._v("Kafka 中数据保存的时间，"),t("strong",[a._v("默认 7 天。")])])]),a._v(" "),t("tr",[t("td",[a._v("log.retention.minutes")]),a._v(" "),t("td",[a._v("Kafka 中数据保存的时间，"),t("strong",[a._v("分钟级别")]),a._v("，默认关闭。")])]),a._v(" "),t("tr",[t("td",[a._v("log.retention.ms")]),a._v(" "),t("td",[a._v("Kafka 中数据保存的时间，"),t("strong",[a._v("毫秒级别")]),a._v("，默认关闭。")])]),a._v(" "),t("tr",[t("td",[a._v("log.retention.check.interval.ms")]),a._v(" "),t("td",[a._v("检查数据是否保存超时的间隔，"),t("strong",[a._v("默认是 5 分钟")]),a._v("。")])]),a._v(" "),t("tr",[t("td",[a._v("log.retention.bytes")]),a._v(" "),t("td",[t("strong",[a._v("默认等于-1，表示无穷大")]),a._v("。超过设置的所有日志总 大小，删除最早的 segment。")])]),a._v(" "),t("tr",[t("td",[a._v("log.cleanup.policy")]),a._v(" "),t("td",[t("strong",[a._v("默认是 delete")]),a._v("，表示所有数据启用删除策略； 如果设置值为 compact，表示所有数据启用压缩策略。")])]),a._v(" "),t("tr",[t("td",[a._v("num.io.threads")]),a._v(" "),t("td",[t("strong",[a._v("默认是 8")]),a._v("。负责写磁盘的线程数。整个参数值要占总核数的 50%。")])]),a._v(" "),t("tr",[t("td",[a._v("num.replica.fetchers")]),a._v(" "),t("td",[a._v("副本拉取线程数，这个参数占总核数的 50%的 1/3")])]),a._v(" "),t("tr",[t("td",[a._v("num.network.threads")]),a._v(" "),t("td",[t("strong",[a._v("默认是 3")]),a._v("。数据传输线程数，这个参数占总核数的 50%的 2/3 。")])]),a._v(" "),t("tr",[t("td",[a._v("log.flush.interval.messages")]),a._v(" "),t("td",[a._v("强制页缓存刷写到磁盘的条数，默认是 long 的最大值，9223372036854775807。一般不建议修改， 交给系统自己管理。")])]),a._v(" "),t("tr",[t("td",[a._v("log.flush.interval.ms")]),a._v(" "),t("td",[a._v("每隔多久，刷数据到磁盘，默认是 null。一般不建 议修改，交给系统自己管理。")])])])]),a._v(" "),t("h2",{attrs:{id:"_3-2-服役新节点-退役旧节点"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-服役新节点-退役旧节点"}},[a._v("#")]),a._v(" 3.2 服役新节点/退役旧节点")]),a._v(" "),t("h2",{attrs:{id:"_3-3-增加分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-增加分区"}},[a._v("#")]),a._v(" 3.3 增加分区")]),a._v(" "),t("h2",{attrs:{id:"_3-4-增加副本因子"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-增加副本因子"}},[a._v("#")]),a._v(" 3.4 增加副本因子")]),a._v(" "),t("h2",{attrs:{id:"_3-5-手动调整分区副本存储"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-5-手动调整分区副本存储"}},[a._v("#")]),a._v(" 3.5 手动调整分区副本存储")]),a._v(" "),t("h2",{attrs:{id:"_3-6-leader-partition-负载平衡"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-6-leader-partition-负载平衡"}},[a._v("#")]),a._v(" 3.6 Leader Partition 负载平衡")]),a._v(" "),t("h2",{attrs:{id:"_3-7-自动创建主题"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-7-自动创建主题"}},[a._v("#")]),a._v(" 3.7 自动创建主题")]),a._v(" "),t("p",[a._v("如果 broker 端配置参数 "),t("strong",[a._v("auto.create.topics.enable")]),a._v(" 设置为 true（默认值是 true），那么当生产者向一个未创建的主题发送消息时，会自动创建一个分区数为 "),t("code",[a._v("num.partitions")]),a._v("（默认值为 1）、副本因子为 default.replication.factor（默认值为 1）的主题。除此之外，当一个消费者开始从未知主题中读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会自动创建一个相应主题。这种创建主题的方式是非预期的，增加了主题管理和维护的难度。 生产环境建议将该参数设置为 false。")]),a._v(" "),t("p",[a._v("1）向一个没有提前创建 five 主题发送数据")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop102 kafka]$ bin/kafka-console-producer.sh --bootstrap-server hadoop102:9092 --topic five\n>hello world\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br"),t("span",{staticClass:"line-number"},[a._v("2")]),t("br")])]),t("p",[a._v("2）查看 five 主题的详情")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --describe --topic five\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("h1",{attrs:{id:"第-4-章-kafka-消费者"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#第-4-章-kafka-消费者"}},[a._v("#")]),a._v(" 第 4 章 Kafka 消费者")]),a._v(" "),t("p",[t("img",{attrs:{src:s(259),alt:"消费者组初始化流程"}})]),a._v(" "),t("h2",{attrs:{id:"_4-6-消费者如何提高吞吐量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-6-消费者如何提高吞吐量"}},[a._v("#")]),a._v(" 4.6 消费者如何提高吞吐量")]),a._v(" "),t("p",[a._v("增加分区数；")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --bootstrap-server hadoop102:9092 --alter --topic first --partitions 3\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("table",[t("thead",[t("tr",[t("th",[a._v("参数名称")]),a._v(" "),t("th",[a._v("描述")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("fetch.max.bytes")]),a._v(" "),t("td",[a._v("默认 "),t("strong",[a._v("Default: 52428800（50 m")]),a._v("）。消费者获取服务器端一批 消息最大的字节数。如果服务器端一批次的数据大于该值 （50m）仍然可以拉取回来这批数据，因此，这不是一个绝对 最大值。一批次的大小受 message.max.bytes （broker config） or max.message.bytes （topic config）影响。")])]),a._v(" "),t("tr",[t("td",[a._v("ax.poll.records")]),a._v(" "),t("td",[a._v("一次 poll 拉取数据返回消息的最大条数，"),t("strong",[a._v("默认是 500 条")])])])])]),a._v(" "),t("h1",{attrs:{id:"第-5-章-kafka-总体"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#第-5-章-kafka-总体"}},[a._v("#")]),a._v(" 第 5 章 Kafka 总体")]),a._v(" "),t("h2",{attrs:{id:"_5-1-如何提升吞吐量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-1-如何提升吞吐量"}},[a._v("#")]),a._v(" 5.1 如何提升吞吐量")]),a._v(" "),t("h4",{attrs:{id:"_1-提升生产吞吐量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-提升生产吞吐量"}},[a._v("#")]),a._v(" 1）提升生产吞吐量")]),a._v(" "),t("ul",[t("li",[a._v("（1）buffer.memory：发送消息的缓冲区大小，默认值是 32m，可以增加到 64m。")]),a._v(" "),t("li",[a._v("（2）batch.size：默认是 16k。如果 batch 设置太小，会导致频繁网络请求，吞吐量下降； 如果 batch 太大，会导致一条消息需要等待很久才能被发送出去，增加网络延时。")]),a._v(" "),t("li",[a._v("（3）linger.ms，这个值默认是 0，意思就是消息必须立即被发送。一般设置一个 5-100 毫秒。如果 linger.ms 设置的太小，会导致频繁网络请求，吞吐量下降；如果 linger.ms 太长， 会导致一条消息需要等待很久才能被发送出去，增加网络延时。")]),a._v(" "),t("li",[a._v("（4）compression.type：默认是 none，不压缩，但是也可以使用 lz4 压缩，效率还是不错的，压缩之后可以减小数据量，提升吞吐量，但是会加大 producer 端的 CPU 开销。")])]),a._v(" "),t("h4",{attrs:{id:"_2-增加分区"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-增加分区"}},[a._v("#")]),a._v(" 2）增加分区")]),a._v(" "),t("h4",{attrs:{id:"_3-消费者提高吞吐量"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-消费者提高吞吐量"}},[a._v("#")]),a._v(" 3）消费者提高吞吐量")]),a._v(" "),t("p",[a._v("（1）调整 fetch.max.bytes 大小，默认是 50m。")]),a._v(" "),t("p",[a._v("（2）调整 max.poll.records 大小，默认是 500 条。")]),a._v(" "),t("h4",{attrs:{id:"_4-增加下游消费者处理能力"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_4-增加下游消费者处理能力"}},[a._v("#")]),a._v(" 4）增加下游消费者处理能力")]),a._v(" "),t("h2",{attrs:{id:"_5-2-数据精准一次"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-2-数据精准一次"}},[a._v("#")]),a._v(" 5.2 数据精准一次")]),a._v(" "),t("h4",{attrs:{id:"_1-生产者角度"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-生产者角度"}},[a._v("#")]),a._v(" 1）生产者角度")]),a._v(" "),t("ul",[t("li",[a._v("acks 设置为-1 （acks=-1）")]),a._v(" "),t("li",[a._v("幂等性（enable.idempotence = true） + 事务 。")])]),a._v(" "),t("h4",{attrs:{id:"_2-broker-服务端角度"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-broker-服务端角度"}},[a._v("#")]),a._v(" 2）broker 服务端角度")]),a._v(" "),t("ul",[t("li",[a._v("分区副本大于等于 2 （--replication-factor 2）。")]),a._v(" "),t("li",[a._v("ISR 里应答的最小副本数量大于等于 2 （min.insync.replicas = 2）。")])]),a._v(" "),t("h2",{attrs:{id:"_3-消费者"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-消费者"}},[a._v("#")]),a._v(" 3）消费者")]),a._v(" "),t("ul",[t("li",[t("strong",[a._v("事务 + 手动提交 offset")]),a._v(" （enable.auto.commit = false）。")]),a._v(" "),t("li",[a._v("消费者输出的目的地必须支持事务（MySQL、Kafka）。")])]),a._v(" "),t("h3",{attrs:{id:"_5-3-合理设置分区数"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-3-合理设置分区数"}},[a._v("#")]),a._v(" 5.3 合理设置分区数")]),a._v(" "),t("ul",[t("li",[a._v("（1）创建一个只有 1 个分区的 topic。")]),a._v(" "),t("li",[a._v("（2）测试这个 topic 的 producer 吞吐量和 consumer 吞吐量。")]),a._v(" "),t("li",[a._v("（3）假设他们的值分别是 Tp 和 Tc，单位可以是 MB/s")]),a._v(" "),t("li",[a._v("（4）然后假设总的目标吞吐量是 Tt，那么分区数 = Tt / min（Tp，Tc）。")])]),a._v(" "),t("p",[a._v("例如：producer 吞吐量 = 20m/s；consumer 吞吐量 = 50m/s，期望吞吐量 100m/s；")]),a._v(" "),t("p",[a._v("分区数 = 100 / 20 = 5 分区")]),a._v(" "),t("p",[a._v("分区数一般设置为：3-10 个")]),a._v(" "),t("p",[a._v("分区数不是越多越好，也不是越少越好，需要搭建完集群，进行压测，再灵活调整分区个数。")]),a._v(" "),t("h2",{attrs:{id:"_5-4-单条日志大于-1m"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-4-单条日志大于-1m"}},[a._v("#")]),a._v(" 5.4 单条日志大于 1m")]),a._v(" "),t("table",[t("thead",[t("tr",[t("th",[a._v("参数名称")]),a._v(" "),t("th",[a._v("描述")])])]),a._v(" "),t("tbody",[t("tr",[t("td",[a._v("message.max.bytes")]),a._v(" "),t("td",[a._v("默认 1m，broker 端接收每个批次消息最大值。")])]),a._v(" "),t("tr",[t("td",[a._v("max.request.size")]),a._v(" "),t("td",[a._v("默认 1m，生产者发往 broker 每个请求消息最大值。针对 topic 级别设置消息体的大小。")])]),a._v(" "),t("tr",[t("td",[a._v("replica.fetch.max.bytes")]),a._v(" "),t("td",[a._v("默认 1m，副本同步数据，每个批次消息最大值。")])]),a._v(" "),t("tr",[t("td",[a._v("fetch.max.bytes")]),a._v(" "),t("td",[a._v("默认 Default: "),t("strong",[a._v("52428800（50 m）")]),a._v("。消费者获取服务器端一批 消息最大的字节数。如果服务器端一批次的数据大于该值 （50m）仍然可以拉取回来这批数据，因此，这不是一个绝对 最大值。一批次的大小受 message.max.bytes （broker config） or max.message.bytes （topic config）影响。")])])])]),a._v(" "),t("h2",{attrs:{id:"_5-5-服务器挂了"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-5-服务器挂了"}},[a._v("#")]),a._v(" 5.5 服务器挂了")]),a._v(" "),t("p",[a._v("在生产环境中，如果某个 Kafka 节点挂掉。")]),a._v(" "),t("p",[a._v("正常处理办法：")]),a._v(" "),t("p",[a._v("（1）先尝试重新启动一下，如果能启动正常，那直接解决。")]),a._v(" "),t("p",[a._v("（2）如果重启不行，考虑增加内存、增加 CPU、网络带宽。")]),a._v(" "),t("p",[a._v("（3）如果将 kafka 整个节点误删除，如果副本数大于等于 2，可以按照服役新节点的方式重新服役一个新节点，并执行负载均衡。")]),a._v(" "),t("h2",{attrs:{id:"_5-6-集群压力测试"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_5-6-集群压力测试"}},[a._v("#")]),a._v(" 5.6 集群压力测试")]),a._v(" "),t("h4",{attrs:{id:"_1-kafka-压测"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_1-kafka-压测"}},[a._v("#")]),a._v(" 1）Kafka 压测")]),a._v(" "),t("p",[a._v("用 Kafka 官方自带的脚本，对 Kafka 进行压测。")]),a._v(" "),t("ul",[t("li",[a._v("生产者压测：kafka-producer-perf-test.sh")]),a._v(" "),t("li",[a._v("消费者压测：kafka-consumer-perf-test.sh")])]),a._v(" "),t("p",[t("img",{attrs:{src:s(1045),alt:"群压力测试"}})]),a._v(" "),t("h4",{attrs:{id:"_2-kafka-producer-压力测试"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_2-kafka-producer-压力测试"}},[a._v("#")]),a._v(" 2）Kafka Producer 压力测试")]),a._v(" "),t("p",[a._v("（1）创建一个 test topic，设置为 3 个分区 3 个副本")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop102 kafka]$ bin/kafka-topics.sh --bootstrapserver hadoop102:9092 --create --replication-factor 3 --partitions 3 --topic test\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("p",[a._v("（2）在/opt/module/kafka/bin 目录下面有这两个文件。我们来测试一下")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop105 kafka]$ bin/kafka-producer-perf-test.sh --topic test --record-size 1024 --num-records 1000000 --throughput 10000 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092,hadoop104:9092 batch.size=16384 linger.ms=0\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("p",[a._v("参数说明：")]),a._v(" "),t("ul",[t("li",[a._v("record-size 是一条信息有多大，单位是字节，本次测试设置为 1k。")]),a._v(" "),t("li",[a._v("num-records 是总共发送多少条信息，本次测试设置为 100 万条。")]),a._v(" "),t("li",[a._v("throughput 是每秒多少条信息，设成-1，表示不限流，尽可能快的生产数据，可测出生产者最大吞吐量。本次实验设置为每秒钟 1 万条。")]),a._v(" "),t("li",[a._v("producer-props 后面可以配置生产者相关参数，batch.size 配置为 16k。")])]),a._v(" "),t("h4",{attrs:{id:"_3-kafka-consumer-压力测试"}},[t("a",{staticClass:"header-anchor",attrs:{href:"#_3-kafka-consumer-压力测试"}},[a._v("#")]),a._v(" 3）Kafka Consumer 压力测试")]),a._v(" "),t("p",[a._v("（1）修改/opt/module/kafka/config/consumer.properties 文件中的一次拉取条数为 500")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("max.poll.records=500\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("p",[a._v("（2）消费 100 万条日志进行压测")]),a._v(" "),t("div",{staticClass:"language- line-numbers-mode"},[t("pre",{pre:!0,attrs:{class:"language-text"}},[t("code",[a._v("[atguigu@hadoop105 kafka]$ bin/kafka-consumer-perf-test.sh --bootstrap-server hadoop102:9092,hadoop103:9092,hadoop104:9092 --topic test --messages 1000000 --consumer.config config/consumer.properties\n")])]),a._v(" "),t("div",{staticClass:"line-numbers-wrapper"},[t("span",{staticClass:"line-number"},[a._v("1")]),t("br")])]),t("p",[a._v("参数说明:")]),a._v(" "),t("ul",[t("li",[a._v("--bootstrap-server 指定 Kafka 集群地址")]),a._v(" "),t("li",[a._v("--topic 指定 topic 的名称")]),a._v(" "),t("li",[a._v("--messages 总共要消费的消息个数。本次实验 100 万条。")])])])}),[],!1,null,null,null);t.default=r.exports},257:function(a,t,s){a.exports=s.p+"assets/img/image-20230920224352147.6f8542b5.png"},258:function(a,t,s){a.exports=s.p+"assets/img/image-20230921001137942.752c52b3.png"},259:function(a,t,s){a.exports=s.p+"assets/img/image-20230925231845686.287b5636.png"}}]);