(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{381:function(a,e,t){a.exports=t.p+"assets/img/image-20230605233218847.bb11179f.png"},382:function(a,e,t){a.exports=t.p+"assets/img/image-20230605233317136.a20ce981.png"},383:function(a,e,t){a.exports=t.p+"assets/img/image-20230605233328124.c7c8c48e.png"},384:function(a,e,t){a.exports=t.p+"assets/img/image-20230605235725066.514cd048.png"},385:function(a,e,t){a.exports=t.p+"assets/img/image-20230606000604200.d43b54bd.png"},497:function(a,e,t){"use strict";t.r(e);var r=t(4),_=Object(r.a)({},(function(){var a=this,e=a._self._c;return e("ContentSlotsDistributor",{attrs:{"slot-key":a.$parent.slotKey}},[e("h1",{attrs:{id:"第一节-mapreduce思想"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#第一节-mapreduce思想"}},[a._v("#")]),a._v(" 第一节 MapReduce思想")]),a._v(" "),e("p",[a._v("MapReduce思想在生活中处处可见。我们或多或少都曾接触过这种思想。MapReduce的思想核心是"),e("strong",[a._v("分而治之，")])]),a._v(" "),e("p",[a._v("充分利用了并行处理的优势。")]),a._v(" "),e("p",[a._v("即使是发布过论文实现分布式计算的谷歌也只是实现了这种思想，而不是自己原创。 MapReduce任务过程是分为两个处理阶段：")]),a._v(" "),e("ul",[e("li",[a._v("Map阶段：Map阶段的主要作用是“分”，即把复杂的任务分解为若干个“简单的任务”来并行处理。 Map阶段的这些任务可以"),e("strong",[a._v("并行计算")]),a._v("，彼此间没有依赖关系。")]),a._v(" "),e("li",[a._v("Reduce阶段：Reduce阶段的主要作用是“合”，即对map阶段的结果进行全局汇总。")])]),a._v(" "),e("p",[a._v("再次理解MapReduce的思想")]),a._v(" "),e("p",[e("img",{attrs:{src:t(381),alt:"image-20230605233218847"}})]),a._v(" "),e("h1",{attrs:{id:"第二节-官方wordcount案例源码解析"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#第二节-官方wordcount案例源码解析"}},[a._v("#")]),a._v(" 第二节 官方WordCount案例源码解析")]),a._v(" "),e("p",[e("img",{attrs:{src:t(382),alt:"image-20230605233317136"}})]),a._v(" "),e("p",[e("img",{attrs:{src:t(383),alt:"image-20230605233328124"}}),e("img",{attrs:{src:t(384),alt:"image-20230605235725066"}})]),a._v(" "),e("p",[a._v("经过查看分析官方WordCount案例源码我们发现一个统计单词数量的MapReduce程序的代码由三个部 分组成，")]),a._v(" "),e("ul",[e("li",[a._v("Mapper类")]),a._v(" "),e("li",[a._v("Reducer类")]),a._v(" "),e("li",[a._v("运行作业的代码（Driver）")])]),a._v(" "),e("p",[a._v("Mapper类继承了org.apache.hadoop.mapreduce.Mapper类重写了其中的map方法，Reducer类继承 了org.apache.hadoop.mapreduce.Reducer类重写了其中的reduce方法。")]),a._v(" "),e("p",[a._v("重写的Map方法作用：map方法其中的逻辑就是用户希望mr程序map阶段如何处理的逻辑；")]),a._v(" "),e("p",[a._v("重写的Reduce方法作用：reduce方法其中的逻辑是用户希望mr程序reduce阶段如何处理的逻辑；")]),a._v(" "),e("h2",{attrs:{id:"_1-hadoop序列化"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_1-hadoop序列化"}},[a._v("#")]),a._v(" 1. Hadoop序列化")]),a._v(" "),e("p",[a._v("为什么进行序列化？")]),a._v(" "),e("p",[a._v("序列化主要是我们通过网络通信传输数据时或者把对象持久化到文件，需要把对象序列化成二进制的结构。")]),a._v(" "),e("p",[a._v("观察源码时发现自定义Mapper类与自定义Reducer类都有泛型类型约束，比如自定义Mapper有四个形参类型，但是形参类型并不是常见的java基本类型。")]),a._v(" "),e("p",[a._v("为什么Hadoop要选择建立自己的序列化格式而不使用java自带serializable？")]),a._v(" "),e("ul",[e("li",[a._v("序列化在分布式程序中非常重要，在Hadoop中，集群中多个节点的进程间的通信是通过RPC（远程过程调用：Remote Procedure Call）实现；RPC将消息序列化成二进制流发送到远程节点，远程节点再将接收到的二进制数据反序列化为原始的消息，因此RPC往往追求如下特点：\n"),e("ul",[e("li",[a._v("紧凑:数据更紧凑，能充分利用网络带宽资源")]),a._v(" "),e("li",[a._v("快速:序列化和反序列化的性能开销更低")])])]),a._v(" "),e("li",[a._v("Hadoop使用的是自己的序列化格式Writable,它比java的序列化serialization更紧凑速度更快。一 个对象使用Serializable序列化后，会携带很多额外信息比如校验信息，Header,继承体系等。")])]),a._v(" "),e("p",[a._v("Java基本类型与Hadoop常用序列化类型")]),a._v(" "),e("table",[e("thead",[e("tr",[e("th",[a._v("Java基本类型")]),a._v(" "),e("th",[a._v("Hadoop Writable类型")])])]),a._v(" "),e("tbody",[e("tr",[e("td",[a._v("boolean")]),a._v(" "),e("td",[a._v("BooleanWritable")])]),a._v(" "),e("tr",[e("td",[a._v("byte")]),a._v(" "),e("td",[a._v("ByteWritable")])]),a._v(" "),e("tr",[e("td",[a._v("int")]),a._v(" "),e("td",[a._v("IntWritable")])]),a._v(" "),e("tr",[e("td",[a._v("float")]),a._v(" "),e("td",[a._v("FloatWritable")])]),a._v(" "),e("tr",[e("td",[a._v("long")]),a._v(" "),e("td",[a._v("LongWritable")])]),a._v(" "),e("tr",[e("td",[a._v("double")]),a._v(" "),e("td",[a._v("DoubleWritable")])]),a._v(" "),e("tr",[e("td",[a._v("String")]),a._v(" "),e("td",[a._v("Text")])]),a._v(" "),e("tr",[e("td",[a._v("map")]),a._v(" "),e("td",[a._v("MapWritable")])]),a._v(" "),e("tr",[e("td",[a._v("array")]),a._v(" "),e("td",[a._v("ArrayWritable")])])])]),a._v(" "),e("h1",{attrs:{id:"第三节-mapreduce编程规范及示例编写"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#第三节-mapreduce编程规范及示例编写"}},[a._v("#")]),a._v(" 第三节 MapReduce编程规范及示例编写")]),a._v(" "),e("h2",{attrs:{id:"_3-1-mapper类"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-mapper类"}},[a._v("#")]),a._v(" 3.1 Mapper类")]),a._v(" "),e("ul",[e("li",[a._v("用户自定义一个Mapper类继承Hadoop的Mapper类")]),a._v(" "),e("li",[a._v("Mapper的输入数据是KV对的形式（类型可以自定义）")]),a._v(" "),e("li",[a._v("Map阶段的业务逻辑定义在map()方法中")]),a._v(" "),e("li",[a._v("Mapper的输出数据是KV对的形式（类型可以自定义）")])]),a._v(" "),e("blockquote",[e("p",[a._v("注意：map()方法是对输入的一个KV对调用一次！！")])]),a._v(" "),e("h2",{attrs:{id:"_3-2-reducer类"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-reducer类"}},[a._v("#")]),a._v(" 3.2 Reducer类")]),a._v(" "),e("ul",[e("li",[a._v("用户自定义Reducer类要继承Hadoop的Reducer类")]),a._v(" "),e("li",[a._v("Reducer的输入数据类型对应Mapper的输出数据类型（KV对）")]),a._v(" "),e("li",[a._v("Reducer的业务逻辑写在reduce()方法中")]),a._v(" "),e("li",[a._v("Reduce()方法是对相同K的一组KV对调用执行一次")])]),a._v(" "),e("h2",{attrs:{id:"_3-3-driver阶段"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-driver阶段"}},[a._v("#")]),a._v(" 3.3 Driver阶段")]),a._v(" "),e("p",[a._v("创建提交YARN集群运行的Job对象，其中封装了MapReduce程序运行所需要的相关参数入输入数据路 径，输出数据路径等，也相当于是一个YARN集群的客户端，主要作用就是提交我们MapReduce程序运行。")]),a._v(" "),e("p",[e("img",{attrs:{src:t(385),alt:"image-20230606000604200"}})]),a._v(" "),e("h2",{attrs:{id:"_3-4-wordcount代码实现"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_3-4-wordcount代码实现"}},[a._v("#")]),a._v(" 3.4 WordCount代码实现")])])}),[],!1,null,null,null);e.default=_.exports}}]);